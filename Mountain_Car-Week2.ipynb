{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvX6rBo1wd0m"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import keras as K\n",
        "\n",
        "import imageio\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env_name = None\n",
        "initial_timestamp = 0.0\n",
        "np.random.seed(1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnCEgKmGwyR6",
        "outputId": "4194fdc7-97ba-460e-a178-09bf70c0fd28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNetwork:\n",
        "\n",
        "    def __init__(self, state_size, action_size, action_high=1.0, action_low=0.0, layer_sizes=(64, 64),\n",
        "                 batch_norm_options=(True, True), dropout_options=(0, 0), learning_rate=0.0001):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.action_high = action_high\n",
        "        self.action_low = action_low\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.batch_norm_options = batch_norm_options\n",
        "        self.dropout_options = dropout_options\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        states = K.layers.Input(shape=(self.state_size,), name='states')\n",
        "        net = states\n",
        "        # hidden layers\n",
        "\n",
        "        for layer_count in range(len(self.layer_sizes)):\n",
        "            net = K.layers.Dense(units=self.layer_sizes[layer_count])(net)\n",
        "            net = K.layers.Activation('relu')(net)\n",
        "            if self.batch_norm_options[layer_count]:\n",
        "                net = K.layers.BatchNormalization()(net)\n",
        "            net = K.layers.Dropout(self.dropout_options[layer_count])(net)\n",
        "\n",
        "        actions = K.layers.Dense(units=self.action_size, activation='linear',\n",
        "                                 name='raw_actions')(net)\n",
        "\n",
        "        self.model = K.models.Model(inputs=states, outputs=actions)\n",
        "\n",
        "        self.optimizer = K.optimizers.Adam(lr=self.learning_rate)\n",
        "        self.model.compile(loss='mse', optimizer=self.optimizer)"
      ],
      "metadata": {
        "id": "WOvMcG1qw0S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "class DDQNAgent:\n",
        "\n",
        "    def __init__(self, env, buffer_size=int(1e5), batch_size=64, gamma=0.99, tau=1e-3, lr=5e-4, callbacks=()):\n",
        "        self.env = env\n",
        "        self.env.seed(1024)\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.Q_targets = 0.0\n",
        "        self.state_size = env.observation_space.shape[0]\n",
        "        self.action_size = env.action_space.n\n",
        "        self.callbacks = callbacks\n",
        "\n",
        "        layer_sizes = [256, 256]\n",
        "        batch_norm_options = [False, False]\n",
        "        dropout_options = [0, 0]\n",
        "\n",
        "        print(\"Initialising DDQN Agent with params : {}\".format(self.__dict__))\n",
        "\n",
        "        # Make local & target model\n",
        "        print(\"Initialising Local DQNetwork\")\n",
        "        self.local_network = DQNetwork(self.state_size, self.action_size,\n",
        "                                       layer_sizes=layer_sizes,\n",
        "                                       batch_norm_options=batch_norm_options,\n",
        "                                       dropout_options=dropout_options,\n",
        "                                       learning_rate=lr)\n",
        "\n",
        "        print(\"Initialising Target DQNetwork\")\n",
        "        self.target_network = DQNetwork(self.state_size, self.action_size,\n",
        "                                        layer_sizes=layer_sizes,\n",
        "                                        batch_norm_options=batch_norm_options,\n",
        "                                        dropout_options=dropout_options,\n",
        "                                        learning_rate=lr)\n",
        "\n",
        "        self.memory = ReplayBuffer(buffer_size=buffer_size, batch_size=batch_size)\n",
        "    def reset_episode(self):\n",
        "        state = self.env.reset()\n",
        "        self.last_state = state\n",
        "        return state\n",
        "\n",
        "    def step(self, action, reward, next_state, done):\n",
        "        self.memory.add(self.last_state, action, reward, next_state, done)\n",
        "\n",
        "        if len(self.memory) > self.batch_size:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, self.gamma)\n",
        "\n",
        "        self.last_state = next_state\n",
        "    def act(self, state, eps=0.):\n",
        "        state = np.reshape(state, [-1, self.state_size])\n",
        "        action = self.local_network.model.predict(state)\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action)\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        for itr in range(len(states)):\n",
        "            state, action, reward, next_state, done = states[itr], actions[itr], rewards[itr], next_states[itr], dones[\n",
        "                itr]\n",
        "            state = np.reshape(state, [-1, self.state_size])\n",
        "            next_state = np.reshape(next_state, [-1, self.state_size])\n",
        "\n",
        "            self.Q_targets = self.local_network.model.predict(state)\n",
        "            if done:\n",
        "                self.Q_targets[0][action] = reward\n",
        "            else:\n",
        "                next_Q_target = self.target_network.model.predict(next_state)[0]\n",
        "                self.Q_targets[0][action] = (reward + gamma * np.max(next_Q_target))\n",
        "\n",
        "            self.local_network.model.fit(state, self.Q_targets, epochs=1, verbose=0, callbacks=self.callbacks)\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_network.model.set_weights(self.local_network.model.get_weights())\n"
      ],
      "metadata": {
        "id": "5jX6hNapw3dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, buffer_size, batch_size):\n",
        "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = np.vstack([e.state for e in experiences if e is not None])\n",
        "        actions = np.vstack([e.action for e in experiences if e is not None])\n",
        "        rewards = np.vstack([e.reward for e in experiences if e is not None])\n",
        "        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n",
        "        dones = np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "X3R-y5KwxuKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9kRLf9vyHPD",
        "outputId": "43616031-7fcc-4829-b5f2-d50a4c36b19d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n"
      ],
      "metadata": {
        "id": "3gD0J66FxxKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(n_episodes=2000, eps_start=1.0, eps_end=0.001, eps_decay=0.9, target_reward=1000):\n",
        "    scores = []\n",
        "    scores_window = deque(maxlen=100)\n",
        "    eps = eps_start\n",
        "    print(\"Starting model training for {} episodes.\".format(n_episodes))\n",
        "    consolidation_counter = 0\n",
        "    for i_episode in range(1, n_episodes + 1):\n",
        "        init_time = time()\n",
        "        state = agent.reset_episode()\n",
        "        score = 0\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                agent.update_target_model()\n",
        "                break\n",
        "        time_taken = time() - init_time\n",
        "        scores_window.append(score)\n",
        "        scores.append(score)\n",
        "        eps = max(eps_end, eps_decay * eps)\n",
        "        print('Episode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}\\tState: {}\\tMean Q-Target: {:.4f}'\n",
        "                     '\\tEffective Epsilon: {:.3f}\\tTime Taken: {:.2f} sec'.format(\n",
        "            i_episode, np.mean(scores_window), score, state[0], np.mean(agent.Q_targets), eps, time_taken))\n",
        "        if i_episode % 100 == 0:\n",
        "            print(\n",
        "                'Episode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}\\tState: {}\\tMean Q-Target: {:.4f}\\tTime Taken: {:.2f} sec '.format(\n",
        "                    i_episode, np.mean(scores_window), score, state[0], np.mean(agent.Q_targets), time_taken))\n",
        "            agent.local_network.model.save('save/{}_local_model_{}.h5'.format(env_name, initial_timestamp))\n",
        "            agent.target_network.model.save('save/{}_target_model_{}.h5'.format(env_name, initial_timestamp))\n",
        "        if np.mean(scores_window) >= target_reward:\n",
        "            consolidation_counter += 1\n",
        "            if consolidation_counter >= 5:\n",
        "                print(\"Completed model training with avg reward {} over last {} episodes.\"\n",
        "                                    \" Training ran for total of {} epsiodes\".format(\n",
        "                    np.mean(scores_window), 100, i_episode))\n",
        "                return scores\n",
        "        else:\n",
        "            consolidation_counter = 0\n",
        "    print(\"Completed model training with avg reward {} over last {} episodes.\"\n",
        "                        \" Training ran for total of {} epsiodes\".format( np.mean(scores_window), 100, n_episodes))\n",
        "    return scores"
      ],
      "metadata": {
        "id": "6TKuqHyKx2A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_model(actor, env_render=False, return_render_img=False):\n",
        "    state = env.reset()\n",
        "    print(\"Start state : {}\".format(state))\n",
        "    score = 0\n",
        "    done = False\n",
        "    images = []\n",
        "    R = 0\n",
        "    t = 0\n",
        "    while not done:\n",
        "        if env_render:\n",
        "            if return_render_img:\n",
        "                images.append(env.render(\"rgb_array\"))\n",
        "            else:\n",
        "                env.render()\n",
        "        state = np.reshape(state, [-1, env.observation_space.shape[0]])\n",
        "        action = actor.predict(state)\n",
        "        next_state, reward, done, _ = env.step(np.argmax(action))\n",
        "        R += reward\n",
        "        t += 1\n",
        "        state = next_state\n",
        "        score += reward\n",
        "        if done:\n",
        "            return score, images\n",
        "    return 0, images"
      ],
      "metadata": {
        "id": "xKZXjkAfyblO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_frames_as_gif(frames):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
        "    display(display_animation(anim, default_mode='loop'))"
      ],
      "metadata": {
        "id": "MSUsKZwYzXSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train\n",
        "env_name = \"MountainCar-v0\"\n",
        "env = gym.make(env_name)\n",
        "agent = DDQNAgent(env, buffer_size=1000, gamma=0.99, batch_size=128, lr=0.001, callbacks=[])\n",
        "scores = train_model(n_episodes=5, target_reward=-110, eps_decay=0.9)\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "plt.plot(np.arange(len(scores)), scores)\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Episode #')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DDGA5hgUzaDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"save/MountainCar-v0_local_model_0.0.h5\"\n",
        "total_iterations = 100\n",
        "expected_reward = -110\n",
        "\n",
        "#Test\n",
        "test_scores = []\n",
        "print(\"Loading the saved model from '{}'\".format(model))\n",
        "actor = K.models.load_model('{}'.format(model))\n",
        "print(\"Now running model test for {} iterations with expected reward >= {}\".format(\n",
        "    total_iterations, expected_reward))\n",
        "frames = play_model(actor, True, True)[1]\n",
        "for itr in range(1, total_iterations + 1):\n",
        "    score = play_model(actor, True)[0]\n",
        "    test_scores.append(score)\n",
        "    print(\"Iteration: {} Score: {}\".format(itr, score))\n",
        "avg_reward = np.mean(test_scores)\n",
        "print(\"Total Avg. Score over {} consecutive iterations : {}\".format(total_iterations,\n",
        "                                                                                 avg_reward))\n",
        "if avg_reward >= expected_reward:\n",
        "    print(\"Env. solved successfully.\")\n",
        "else:\n",
        "    print(\"Agent failed to solve the env.\")"
      ],
      "metadata": {
        "id": "dRU_08_EzdMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation\n",
        "import numpy as np\n",
        "from IPython.display import HTML\n",
        "\n",
        "plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
        "patch = plt.imshow(frames[0])\n",
        "plt.axis('off')\n",
        "animate = lambda i: patch.set_data(frames[i])\n",
        "ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
        "HTML(ani.to_jshtml())"
      ],
      "metadata": {
        "id": "SlKAj8pg0uyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OTmYNXKJ0yxO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}